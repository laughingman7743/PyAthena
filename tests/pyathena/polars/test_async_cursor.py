# -*- coding: utf-8 -*-
import contextlib
import random
import string
import time
from datetime import datetime
from random import randint

import polars as pl
import pytest

from pyathena.error import NotSupportedError, ProgrammingError
from pyathena.model import AthenaQueryExecution
from pyathena.polars.async_cursor import AsyncPolarsCursor
from pyathena.result_set import AthenaResultSet
from tests import ENV
from tests.pyathena.conftest import connect


class TestAsyncPolarsCursor:
    @pytest.mark.parametrize(
        "async_polars_cursor",
        [{"cursor_kwargs": {"unload": False}}, {"cursor_kwargs": {"unload": True}}],
        indirect=["async_polars_cursor"],
    )
    def test_fetchone(self, async_polars_cursor):
        query_id, future = async_polars_cursor.execute("SELECT * FROM one_row")
        result_set = future.result()
        assert result_set.rownumber == 0
        assert result_set.fetchone() == (1,)
        assert result_set.rownumber == 1
        assert result_set.fetchone() is None

    @pytest.mark.parametrize(
        "async_polars_cursor",
        [{"cursor_kwargs": {"unload": False}}, {"cursor_kwargs": {"unload": True}}],
        indirect=["async_polars_cursor"],
    )
    def test_fetchmany(self, async_polars_cursor):
        query_id, future = async_polars_cursor.execute("SELECT * FROM many_rows LIMIT 15")
        result_set = future.result()
        assert len(result_set.fetchmany(10)) == 10
        assert len(result_set.fetchmany(10)) == 5

    @pytest.mark.parametrize(
        "async_polars_cursor",
        [{"cursor_kwargs": {"unload": False}}, {"cursor_kwargs": {"unload": True}}],
        indirect=["async_polars_cursor"],
    )
    def test_fetchall(self, async_polars_cursor):
        query_id, future = async_polars_cursor.execute("SELECT * FROM one_row")
        result_set = future.result()
        assert result_set.fetchall() == [(1,)]
        query_id, future = async_polars_cursor.execute("SELECT a FROM many_rows ORDER BY a")
        result_set = future.result()
        if async_polars_cursor._unload:
            assert sorted(result_set.fetchall()) == [(i,) for i in range(10000)]
        else:
            assert result_set.fetchall() == [(i,) for i in range(10000)]

    @pytest.mark.parametrize(
        "async_polars_cursor",
        [{"cursor_kwargs": {"unload": False}}, {"cursor_kwargs": {"unload": True}}],
        indirect=["async_polars_cursor"],
    )
    def test_iterator(self, async_polars_cursor):
        query_id, future = async_polars_cursor.execute("SELECT * FROM one_row")
        result_set = future.result()
        assert list(result_set) == [(1,)]
        pytest.raises(StopIteration, result_set.__next__)

    @pytest.mark.parametrize(
        "async_polars_cursor",
        [{"cursor_kwargs": {"unload": False}}, {"cursor_kwargs": {"unload": True}}],
        indirect=["async_polars_cursor"],
    )
    def test_arraysize(self, async_polars_cursor):
        async_polars_cursor.arraysize = 5
        query_id, future = async_polars_cursor.execute("SELECT * FROM many_rows LIMIT 20")
        result_set = future.result()
        assert len(result_set.fetchmany()) == 5

    def test_arraysize_default(self, async_polars_cursor):
        assert async_polars_cursor.arraysize == AthenaResultSet.DEFAULT_FETCH_SIZE

    def test_invalid_arraysize(self, async_polars_cursor):
        async_polars_cursor.arraysize = 10000
        assert async_polars_cursor.arraysize == 10000
        with pytest.raises(ProgrammingError):
            async_polars_cursor.arraysize = -1

    @pytest.mark.parametrize(
        "async_polars_cursor",
        [{"cursor_kwargs": {"unload": False}}, {"cursor_kwargs": {"unload": True}}],
        indirect=["async_polars_cursor"],
    )
    def test_description(self, async_polars_cursor):
        query_id, future = async_polars_cursor.execute(
            "SELECT CAST(1 AS INT) AS foobar FROM one_row"
        )
        result_set = future.result()
        assert result_set.fetchall() == [(1,)]
        if async_polars_cursor._unload:
            assert result_set.description == [("foobar", "integer", None, None, 10, 0, "NULLABLE")]
        else:
            assert result_set.description == [("foobar", "integer", None, None, 10, 0, "UNKNOWN")]

        future = async_polars_cursor.description(query_id)
        description = future.result()
        assert result_set.description == description

    @pytest.mark.parametrize(
        "async_polars_cursor",
        [{"cursor_kwargs": {"unload": False}}, {"cursor_kwargs": {"unload": True}}],
        indirect=["async_polars_cursor"],
    )
    def test_query_execution(self, async_polars_cursor):
        query = "SELECT * FROM one_row"
        query_id, future = async_polars_cursor.execute(query)
        result_set = future.result()

        future = async_polars_cursor.query_execution(query_id)
        query_execution = future.result()

        assert query_execution.database == ENV.schema
        assert query_execution.catalog
        assert query_execution.query_id
        if async_polars_cursor._unload:
            assert query_execution.query.startswith("UNLOAD")
            assert query in query_execution.query
        else:
            assert query_execution.query == query
        assert query_execution.statement_type == AthenaQueryExecution.STATEMENT_TYPE_DML
        assert query_execution.work_group == ENV.default_work_group
        assert query_execution.state == AthenaQueryExecution.STATE_SUCCEEDED
        assert query_execution.state_change_reason is None
        assert query_execution.submission_date_time
        assert isinstance(query_execution.submission_date_time, datetime)
        assert query_execution.completion_date_time
        assert isinstance(query_execution.completion_date_time, datetime)
        assert query_execution.data_scanned_in_bytes
        assert query_execution.engine_execution_time_in_millis
        assert query_execution.query_queue_time_in_millis
        assert query_execution.total_execution_time_in_millis
        assert query_execution.output_location
        assert query_execution.encryption_option is None
        assert query_execution.kms_key is None
        assert query_execution.selected_engine_version
        assert query_execution.effective_engine_version

        assert result_set.database == query_execution.database
        assert result_set.catalog == query_execution.catalog
        assert result_set.query_id == query_execution.query_id
        assert result_set.query == query_execution.query
        assert result_set.statement_type == query_execution.statement_type
        assert result_set.work_group == query_execution.work_group
        assert result_set.state == query_execution.state
        assert result_set.state_change_reason == query_execution.state_change_reason
        assert result_set.submission_date_time == query_execution.submission_date_time
        assert result_set.completion_date_time == query_execution.completion_date_time
        assert result_set.data_scanned_in_bytes == query_execution.data_scanned_in_bytes
        assert (
            result_set.engine_execution_time_in_millis
            == query_execution.engine_execution_time_in_millis
        )
        assert result_set.query_queue_time_in_millis == query_execution.query_queue_time_in_millis
        assert (
            result_set.total_execution_time_in_millis
            == query_execution.total_execution_time_in_millis
        )
        assert (
            result_set.query_planning_time_in_millis
            == query_execution.query_planning_time_in_millis
        )
        assert (
            result_set.service_processing_time_in_millis
            == query_execution.service_processing_time_in_millis
        )
        assert result_set.output_location == query_execution.output_location
        assert result_set.data_manifest_location == query_execution.data_manifest_location
        assert result_set.encryption_option == query_execution.encryption_option
        assert result_set.kms_key == query_execution.kms_key
        assert result_set.selected_engine_version == query_execution.selected_engine_version
        assert result_set.effective_engine_version == query_execution.effective_engine_version

    @pytest.mark.parametrize(
        "async_polars_cursor",
        [{"cursor_kwargs": {"unload": False}}, {"cursor_kwargs": {"unload": True}}],
        indirect=["async_polars_cursor"],
    )
    def test_poll(self, async_polars_cursor):
        query_id, _ = async_polars_cursor.execute("SELECT * FROM one_row")
        future = async_polars_cursor.poll(query_id)
        query_execution = future.result()
        assert query_execution.state in [
            AthenaQueryExecution.STATE_QUEUED,
            AthenaQueryExecution.STATE_RUNNING,
            AthenaQueryExecution.STATE_SUCCEEDED,
            AthenaQueryExecution.STATE_FAILED,
            AthenaQueryExecution.STATE_CANCELLED,
        ]

    @pytest.mark.parametrize(
        "async_polars_cursor",
        [{"cursor_kwargs": {"unload": False}}, {"cursor_kwargs": {"unload": True}}],
        indirect=["async_polars_cursor"],
    )
    def test_bad_query(self, async_polars_cursor):
        query_id, future = async_polars_cursor.execute(
            "SELECT does_not_exist FROM this_really_does_not_exist"
        )
        result_set = future.result()
        assert result_set.state == AthenaQueryExecution.STATE_FAILED
        assert result_set.state_change_reason is not None
        assert result_set.error_category is not None
        assert result_set.error_type is not None

    @pytest.mark.parametrize(
        "async_polars_cursor",
        [{"cursor_kwargs": {"unload": False}}, {"cursor_kwargs": {"unload": True}}],
        indirect=["async_polars_cursor"],
    )
    def test_as_polars(self, async_polars_cursor):
        query_id, future = async_polars_cursor.execute("SELECT * FROM one_row")
        assert query_id is not None
        df = future.result().as_polars()
        assert isinstance(df, pl.DataFrame)
        assert df.height == 1
        assert df.width == 1
        assert df.to_dicts() == [{"number_of_rows": 1}]

    @pytest.mark.parametrize(
        "async_polars_cursor",
        [{"cursor_kwargs": {"unload": False}}, {"cursor_kwargs": {"unload": True}}],
        indirect=["async_polars_cursor"],
    )
    def test_many_as_polars(self, async_polars_cursor):
        query_id, future = async_polars_cursor.execute("SELECT * FROM many_rows")
        assert query_id is not None
        df = future.result().as_polars()
        assert isinstance(df, pl.DataFrame)
        assert df.height == 10000
        assert df.width == 1

    @pytest.mark.parametrize(
        "async_polars_cursor",
        [{"cursor_kwargs": {"unload": False}}, {"cursor_kwargs": {"unload": True}}],
        indirect=["async_polars_cursor"],
    )
    def test_as_arrow(self, async_polars_cursor):
        query_id, future = async_polars_cursor.execute("SELECT * FROM one_row")
        assert query_id is not None
        table = future.result().as_arrow()
        assert table.num_rows == 1
        assert table.num_columns == 1

    def test_cancel(self, async_polars_cursor):
        query_id, future = async_polars_cursor.execute(
            """
            SELECT a.a * rand(), b.a * rand()
            FROM many_rows a
            CROSS JOIN many_rows b
            """
        )
        time.sleep(randint(5, 10))
        async_polars_cursor.cancel(query_id)
        result_set = future.result()
        assert result_set.state == AthenaQueryExecution.STATE_CANCELLED
        assert result_set.description is None
        assert result_set.fetchone() is None
        assert result_set.fetchmany() == []
        assert result_set.fetchall() == []

    def test_open_close(self):
        with contextlib.closing(connect()) as conn, conn.cursor(AsyncPolarsCursor):
            pass

    def test_no_ops(self):
        conn = connect()
        cursor = conn.cursor(AsyncPolarsCursor)
        pytest.raises(NotSupportedError, lambda: cursor.executemany("SELECT * FROM one_row", []))
        cursor.close()
        conn.close()

    @pytest.mark.parametrize(
        "async_polars_cursor",
        [{"cursor_kwargs": {"unload": False}}, {"cursor_kwargs": {"unload": True}}],
        indirect=["async_polars_cursor"],
    )
    def test_empty_result(self, async_polars_cursor):
        table = "test_polars_cursor_empty_result_" + "".join(
            random.choices(string.ascii_lowercase + string.digits, k=10)
        )
        query_id, future = async_polars_cursor.execute(
            f"""
            CREATE EXTERNAL TABLE IF NOT EXISTS
            {ENV.schema}.{table} (number_of_rows INT)
            ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
            LINES TERMINATED BY '\n' STORED AS TEXTFILE
            LOCATION '{ENV.s3_staging_dir}{ENV.schema}/{table}/'
            """
        )
        assert query_id is not None
        df = future.result().as_polars()
        assert df.height == 0
        assert df.width == 0

    @pytest.mark.parametrize(
        "async_polars_cursor",
        [
            {
                "cursor_kwargs": {"unload": True},
            },
        ],
        indirect=["async_polars_cursor"],
    )
    def test_empty_result_unload(self, async_polars_cursor):
        query_id, future = async_polars_cursor.execute(
            """
            SELECT * FROM one_row LIMIT 0
            """
        )
        assert query_id is not None
        df = future.result().as_polars()
        assert df.height == 0
        assert df.width == 0

    def test_iter_chunks(self):
        """Test chunked iteration over query results."""
        with contextlib.closing(connect(schema_name=ENV.schema)) as conn:
            cursor = conn.cursor(AsyncPolarsCursor, chunksize=5)
            query_id, future = cursor.execute("SELECT * FROM many_rows LIMIT 15")
            assert query_id is not None
            result_set = future.result()
            chunks = list(result_set.iter_chunks())
            assert len(chunks) > 0
            total_rows = sum(chunk.height for chunk in chunks)
            assert total_rows == 15
            for chunk in chunks:
                assert isinstance(chunk, pl.DataFrame)

    def test_iter_chunks_without_chunksize(self, async_polars_cursor):
        """Test that iter_chunks raises ProgrammingError when chunksize is not set."""
        query_id, future = async_polars_cursor.execute("SELECT * FROM one_row")
        assert query_id is not None
        result_set = future.result()
        with pytest.raises(ProgrammingError) as exc_info:
            list(result_set.iter_chunks())
        assert "chunksize must be set" in str(exc_info.value)

    def test_iter_chunks_many_rows(self):
        """Test chunked iteration with many rows."""
        with contextlib.closing(connect(schema_name=ENV.schema)) as conn:
            cursor = conn.cursor(AsyncPolarsCursor, chunksize=1000)
            query_id, future = cursor.execute("SELECT * FROM many_rows")
            assert query_id is not None
            result_set = future.result()
            chunks = list(result_set.iter_chunks())
            total_rows = sum(chunk.height for chunk in chunks)
            assert total_rows == 10000
            assert len(chunks) >= 10  # At least 10 chunks with chunksize=1000

    @pytest.mark.parametrize(
        "async_polars_cursor",
        [
            {
                "cursor_kwargs": {"unload": True, "chunksize": 5},
            },
        ],
        indirect=["async_polars_cursor"],
    )
    def test_iter_chunks_unload(self, async_polars_cursor):
        """Test chunked iteration with UNLOAD (Parquet)."""
        query_id, future = async_polars_cursor.execute("SELECT * FROM many_rows LIMIT 15")
        assert query_id is not None
        result_set = future.result()
        chunks = list(result_set.iter_chunks())
        assert len(chunks) > 0
        total_rows = sum(chunk.height for chunk in chunks)
        assert total_rows == 15
        for chunk in chunks:
            assert isinstance(chunk, pl.DataFrame)

    def test_iter_chunks_data_consistency(self):
        """Test that chunked and regular reading produce the same data."""
        with contextlib.closing(connect(schema_name=ENV.schema)) as conn:
            # Regular reading (no chunksize)
            regular_cursor = conn.cursor(AsyncPolarsCursor)
            query_id, future = regular_cursor.execute("SELECT * FROM many_rows LIMIT 100")
            assert query_id is not None
            regular_df = future.result().as_polars()

            # Chunked reading
            chunked_cursor = conn.cursor(AsyncPolarsCursor, chunksize=25)
            query_id, future = chunked_cursor.execute("SELECT * FROM many_rows LIMIT 100")
            assert query_id is not None
            result_set = future.result()
            chunked_dfs = list(result_set.iter_chunks())

            # Combine chunks
            combined_df = pl.concat(chunked_dfs)

            # Should have the same data (sort for comparison)
            assert regular_df.sort("a").equals(combined_df.sort("a"))

            # Should have multiple chunks
            assert len(chunked_dfs) > 1

    def test_iter_chunks_chunk_sizes(self):
        """Test that chunks have correct sizes."""
        with contextlib.closing(connect(schema_name=ENV.schema)) as conn:
            cursor = conn.cursor(AsyncPolarsCursor, chunksize=10)
            query_id, future = cursor.execute("SELECT * FROM many_rows LIMIT 50")
            assert query_id is not None
            result_set = future.result()

            chunk_sizes = []
            total_rows = 0

            for chunk in result_set.iter_chunks():
                chunk_size = chunk.height
                chunk_sizes.append(chunk_size)
                total_rows += chunk_size

                # Each chunk should not exceed chunksize
                assert chunk_size <= 10

            # Should have processed all 50 rows
            assert total_rows == 50

            # Should have multiple chunks
            assert len(chunk_sizes) > 1
